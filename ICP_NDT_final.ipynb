{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#import libraries and frameworks\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import exercises.tools.utils as utils\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from IPython.display import display, Math, Latex, Markdown, HTML\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDT\n",
    "\n",
    "The normal distribution transform calculates the probability densitiy function based on the normal distribution for the points to estimatate the likelyhood of sampling a point at given area in the pointcloud, this is done piecewise by dividing the pointcloud map into a grid. that a point belongs into certain grid cell. The transformation from one pointcloud to another becomes a statistical problem and\n",
    "can now be solved using e.g.: maximum likelyhood optimization. One possibility is to use the newton alogrithm to optimize the function\n",
    "and find the transform.\n",
    "\n",
    "### Module\n",
    "In this implementation the poincloud library (https://pointclouds.org/) was used, with python bindings from (https://github.com/hummat/registration).\n",
    "The bindings were slightly adopted for our use case and the pcl110 registration library was used.\n",
    "\n",
    "The module uses mostly default parameter except the following:\n",
    "- epsilon: was chosen for 0.001 to be in line with the assignment\n",
    "- downsample: was chosen as 0.5 to speed up the transform calculation\n",
    "- voxelize: is set to 0.2 to help again with computation time\n",
    "\n",
    "The distance threshold and inlier threshold had no measerable effect on the performance when tuning it within reasonable ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGLIB loaded\n",
      "<CDLL '/app/asdas/libregistration_pcl110.so', handle 4d9cfd0 at 0x7f4b753ccf10>\n"
     ]
    }
   ],
   "source": [
    "#lib wrapper for cpp registration libary slightly altered from: https://github.com/hummat/registration\n",
    "#using registration library pcl110\n",
    "\n",
    "import os\n",
    "import ctypes\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_library(path: str = os.getcwd(), name: str = \"libregistration_pcl110\") -> None:\n",
    "    global REGLIB\n",
    "    try:\n",
    "        REGLIB = np.ctypeslib.load_library(libname=name, loader_path=path)\n",
    "        print(\"REGLIB loaded\")\n",
    "        print(REGLIB)\n",
    "    except OSError:\n",
    "        print(\"Compiled C++ library was not found in the current directory. Please use `load_library` to load it from \"\n",
    "              \"a custom directory, then ignore this message.\")\n",
    "\n",
    "\n",
    "load_library()\n",
    "\n",
    "\n",
    "def load_data(path: str, delimiter: str = ' ') -> np.ndarray:\n",
    "    \"\"\"Loads point cloud data of type `CSV`, `PLY` and `PCD`.\n",
    "\n",
    "    The file should contain one point per line where each number is separated by the `delimiter` character.\n",
    "    Any none numeric lines will be skipped.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the file.\n",
    "        delimiter (char): Separation of numbers in each line of the file.\n",
    "\n",
    "    Returns:\n",
    "        A ndarray of shape NxD where `N` are the number of points in the point cloud and `D` their dimension.\n",
    "    \"\"\"\n",
    "    data = list()\n",
    "    with open(path, newline='\\n') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        lines = 0\n",
    "        skips = 0\n",
    "        while True:\n",
    "            try:\n",
    "                row = next(reader)\n",
    "                row = [x for x in row if not isinstance(x, str)]\n",
    "                if len(row) in [3, 6, 9]:\n",
    "                    data.append(row[:3])\n",
    "                else:\n",
    "                    skips += 1\n",
    "            except ValueError:\n",
    "                skips += 1\n",
    "                pass\n",
    "            except StopIteration:\n",
    "                print(f\"Found {lines} lines. Skipped {skips}. Loaded {lines - skips} points.\")\n",
    "                break\n",
    "            lines += 1\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def set_argtypes(algorithm, source, target):\n",
    "    \"\"\"Tells the underlying C++ code which data types and dimensions to expect.\n",
    "\n",
    "    Args:\n",
    "        algorithm (str): The registration algorithm to use. One of `icp` or `ndt`.\n",
    "        source (ndarray): The source point cloud.\n",
    "        target (ndarray): The target point cloud.\n",
    "    \"\"\"\n",
    "    REGLIB.icp.restype = ctypes.c_double\n",
    "    REGLIB.ndt.restype = ctypes.c_double\n",
    "    argtypes = [np.ctypeslib.ndpointer(dtype=np.float64, ndim=source.ndim, shape=source.shape,\n",
    "                                       flags='C_CONTIGUOUS'), ctypes.c_size_t,\n",
    "                np.ctypeslib.ndpointer(dtype=np.float64, ndim=target.ndim, shape=target.shape,\n",
    "                                       flags='C_CONTIGUOUS'), ctypes.c_size_t,\n",
    "                np.ctypeslib.ndpointer(dtype=np.float64, ndim=2, shape=(4, 4), flags='C_CONTIGUOUS'),\n",
    "                ctypes.c_int, ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_double, ctypes.c_bool]\n",
    "    if algorithm == 'icp':\n",
    "        REGLIB.icp.argtypes = argtypes\n",
    "    elif algorithm == 'ndt':\n",
    "        argtypes.extend([ctypes.c_float, ctypes.c_double, ctypes.c_float])\n",
    "        REGLIB.ndt.argtypes = argtypes\n",
    "\n",
    "\n",
    "def icp(source,\n",
    "        target,\n",
    "        transformation,\n",
    "        nr_iterations=25,\n",
    "        distance_threshold=1.0,\n",
    "        epsilon=1e-6,\n",
    "        inlier_threshold=0.0005,\n",
    "        downsample=0,\n",
    "        visualize=False):\n",
    "    \"\"\"The `Iterative Closest Point` (ICP) algorithm.\n",
    "\n",
    "    Args:\n",
    "        source (ndarray): The point cloud that we want to align to the target.\n",
    "        target (ndarray): The point cloud that the source is aligned to.\n",
    "        nr_iterations (int): The maximum number of iterations the internal optimization should run for.\n",
    "        distance_threshold (float): The maximum distance threshold between two correspondent points in\n",
    "                                    source -> target. If the distance is larger than this threshold, the points will\n",
    "                                    be ignored in the alignment process.\n",
    "        epsilon (float): The transformation epsilon (maximum allowable difference between two consecutive\n",
    "                 transformations) in order for an optimization to be considered as having converged to the final\n",
    "                 solution.\n",
    "        inlier_threshold (float): The inlier distance threshold for the internal RANSAC outlier rejection loop.\n",
    "                          The method considers a point to be an inlier, if the distance between the target data\n",
    "                          index and the transformed source index is smaller than the given inlier distance\n",
    "                          threshold.\n",
    "        downsample (float): Assembles a local 3D grid over a given PointCloud and downsamples + filters the data.\n",
    "        visualize (bool): Can be used to visualize and control the progress of the algorithm.\n",
    "\n",
    "    Returns:\n",
    "        A ndarray with the final transformation matrix between source and target.\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "    \n",
    "    set_argtypes('icp', source, target)\n",
    "    score = REGLIB.icp(source, len(source), target, len(target), transformation,\n",
    "                       nr_iterations, distance_threshold, epsilon, inlier_threshold, downsample, visualize)\n",
    "    print(f\"ICP converged. Fitness score: {score:.2f}\") if score > 0 else print(\"ICP did not converge!\")\n",
    "    return transformation\n",
    "\n",
    "\n",
    "def ndt(source,\n",
    "        target,\n",
    "        nr_iterations=25,\n",
    "        distance_threshold=1.0,\n",
    "        epsilon=0.01,\n",
    "        inlier_threshold=0.05,\n",
    "        downsample=0,\n",
    "        visualize=False,\n",
    "        resolution=1.0,\n",
    "        step_size=0.1,\n",
    "        voxelize=0,\n",
    "        transformation=np.identity(4)):\n",
    "    \"\"\"The `Normal Distributions Transform` (NDT) algorithm.\n",
    "\n",
    "    Args:\n",
    "        source (ndarray): The point cloud that we want to align to the target.\n",
    "        target (ndarray): The point cloud that the source is aligned to.\n",
    "        nr_iterations (int): The maximum number of iterations the internal optimization should run for.\n",
    "        distance_threshold (float): The maximum distance threshold between two correspondent points in\n",
    "                                    source -> target. If the distance is larger than this threshold, the points will\n",
    "                                    be ignored in the alignment process.\n",
    "        epsilon (float): The transformation epsilon (maximum allowable difference between two consecutive\n",
    "                 transformations) in order for an optimization to be considered as having converged to the final\n",
    "                 solution.\n",
    "        inlier_threshold (float): The inlier distance threshold for the internal RANSAC outlier rejection loop.\n",
    "                          The method considers a point to be an inlier, if the distance between the target data\n",
    "                          index and the transformed source index is smaller than the given inlier distance\n",
    "                          threshold.\n",
    "        downsample (float): Assembles a local 3D grid over a given PointCloud and downsamples + filters the data.\n",
    "        visualize (bool): Can be used to visualize and control the progress of the algorithm.\n",
    "        resolution (float): The resolution of the voxel grid. Try increasing this in case of core dumps.\n",
    "        step_size (float): The Newton line search maximum step length.\n",
    "        voxelize (bool): If set to `True`, the source cloud is converted into a voxel model before alignment.\n",
    "\n",
    "    Returns:\n",
    "        A ndarray with the final transformation matrix between source and target.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    set_argtypes('ndt', source, target)\n",
    "    score = REGLIB.ndt(source, len(source), target, len(target), transformation,\n",
    "                  nr_iterations, distance_threshold, epsilon, inlier_threshold, downsample, visualize,\n",
    "                  resolution, step_size, voxelize)\n",
    "    #print(f\"NDT converged. Fitness score: {score:.2f}\") if score > 0 else print(\"NDT did not converge!\")\n",
    "    return transformation, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of voxel filter using open3d to downsample points\n",
    "def voxel_downsample(points, voxel_size):\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "\n",
    "    frame_cloud_downsampled = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    return np.array(frame_cloud_downsampled.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 143960 lines. Skipped 11. Loaded 143949 points.\n"
     ]
    }
   ],
   "source": [
    "#Load source frames, target map and ground truth\n",
    "frames_dir = \"/app/dataset/frames\"\n",
    "frame_files = os.listdir(frames_dir)\n",
    "#frames = [o3d.io.read_point_cloud(os.path.join(frames_dir, f)) for f in frame_files]\n",
    "\n",
    "\n",
    "#load ground truth csv as pandas dataframe\n",
    "g_df = pd.read_csv(\"/app/dataset/ground_truth.csv\")\n",
    "samples = len(frame_files)\n",
    "\n",
    "\n",
    "#Load target map points\n",
    "target_points = load_data(\"/app/dataset/map.pcd\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICP localization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial transformation\n",
    "initTransform = np.identity(4)\n",
    "icp_errors =[]\n",
    "voxel_size = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#loops over all frames and returns the the lateral errors \n",
    "for sample in tqdm(range(samples)):\n",
    "\n",
    "    frame_path = os.path.join(frames_dir, frame_files[sample])\n",
    "    source_points = load_data(frame_path)\n",
    "    \n",
    "    \n",
    "    #voxel filter\n",
    "    points = voxel_downsample(source_points, 0.2)\n",
    "\n",
    "\n",
    "    #transfrom source points based on last alignment transformation\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd = pcd.transform(initTransform)\n",
    "    t_points = np.array(pcd.points)\n",
    "\n",
    "    #ICP alignmnet returns transformation matrix\n",
    "\n",
    "\n",
    "    \n",
    "    trans= icp(source= t_points, target=target_points,transformation=np.identity(4), nr_iterations=30, epsilon=1e-6,\n",
    "                         inlier_threshold=1e-6, distance_threshold= 1, downsample = 0, visualize=False)\n",
    "    \n",
    "\n",
    "    initTransform = trans * initTransform\n",
    "\n",
    "    \n",
    "    #grab frame values from ground truth\n",
    "    frame_gt = g_df[g_df['Frame'] == sample]\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate L2 norm for latent error \n",
    "    errors = np.linalg.norm(np.array([initTransform[0,3],initTransform[1,3],initTransform[2,3]])-np.array([ frame_gt[' x'].item(),  frame_gt[' y'].item(),  frame_gt[' z'].item()]))  \n",
    "    print('Frame :', sample)\n",
    "    print(\"Error--------:\", errors) \n",
    "    if errors>1.2:\n",
    "        print(\"Latent error above the threshold of 1.2\")\n",
    "\n",
    "  \n",
    "    initTransform = trans \n",
    "\n",
    "    icp_errors.append(errors)\n",
    "\n",
    "end_time = time.time()\n",
    "total_icp_time = end_time - start_time\n",
    "\n",
    "print(f\"Total execution time: {total_icp_time:.3f} seconds\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NDT LOCALIZATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1014 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9104 lines. Skipped 11. Loaded 9093 points.\n"
     ]
    }
   ],
   "source": [
    "#initial transformation\n",
    "initTransform = np.identity(4)\n",
    "ndt_errors =[]\n",
    "voxel_size = 0.2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#loops over all frames and returns the the lateral errors \n",
    "for sample in tqdm(range(samples)):\n",
    "\n",
    "    frame_path = os.path.join(\"/app/dataset/frames/\", frame_files[sample])\n",
    "    source_points = load_data(frame_path)\n",
    "    \n",
    "    \n",
    "    #voxel filter\n",
    "    points = voxel_downsample(source_points, 0.2)\n",
    "\n",
    "\n",
    "    #transfrom source points based on last alignment transformation\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd = pcd.transform(initTransform)\n",
    "    t_points = np.array(pcd.points)\n",
    "\n",
    "    #NDT alignmnet returns transformation matrix\n",
    "\n",
    "    trans= ndt(source= t_points, target=target_points,transformation=np.identity(4),voxelize=0, nr_iterations=60, epsilon=1e-3,\n",
    "                         inlier_threshold=1e-3, distance_threshold= 1, downsample = 0, visualize=False)\n",
    "    \n",
    "\n",
    "    initTransform = trans * initTransform\n",
    "\n",
    "    \n",
    "    #grab frame values from ground truth\n",
    "    frame_gt = g_df[g_df['Frame'] == sample]\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate L2 norm for latent error \n",
    "    errors = np.linalg.norm(np.array([initTransform[0,3],initTransform[1,3],initTransform[2,3]])-np.array([ frame_gt[' x'].item(),  frame_gt[' y'].item(),  frame_gt[' z'].item()]))  \n",
    "    print('Frame :', sample)\n",
    "    print(\"Error--------:\", errors) \n",
    "    if errors>1.2:\n",
    "        print(\"Too big error\")\n",
    "\n",
    "\n",
    "    initTransform = trans \n",
    "\n",
    "    ndt_errors.append(errors)\n",
    "\n",
    "end_time = time.time()\n",
    "total_ndt_time = end_time - start_time\n",
    "\n",
    "print(f\"Total execution time: {total_ndt_time:.3f} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "### RMSE\n",
    "\n",
    "### Runtime\n",
    "Without any downsampling the *ndt* needs around 15 seconds for the calculation of one transform from our data set. With the chosen downsampling \n",
    "and voxelization parameter, the calculation takes only roughly one second. While trying the algorithm on not downsampled data samples,\n",
    "no performance difference could be observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sucessful frames localization using ICP : 32\n",
      "Mean error using ICP localization : 10.237\n",
      "Max error using ICP localization : 24.898\n"
     ]
    }
   ],
   "source": [
    "error_icp = np.array(icp_errors)\n",
    "Mean_icp_error = np.mean(error_icp)\n",
    "Max_icp_error = np.max(error_icp)\n",
    "\n",
    "print(\"Number of sucessful frames localization using ICP :\", len(error_icp[error_icp<1.2]))\n",
    "print(\"Mean error using ICP localization :\",f'{Mean_icp_error:.3f}')\n",
    "print(\"Max error using ICP localization :\", f'{Max_icp_error:.3f}')\n",
    "print(\"Time needed for localization of all frames using ICP :\", f'{total_icp_time:.3f}', \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ndt = np.array(ndt_errors)\n",
    "Mean_ndt_error = np.mean(error_ndt)\n",
    "Max_ndt_error = np.max(error_ndt)\n",
    "\n",
    "print(\"Number of failed frames localization using NDT :\", len(error_ndt[error_ndt<1.2]))\n",
    "print(\"Mean error using NDT localization :\", f'{Mean_ndt_error:.3f}')\n",
    "print(\"Max error using NDT localization :\", f'{Max_ndt_error:.3f}')\n",
    "print(\"Time needed for localization of all frames using ndt :\", f'{total_ndt_time:.3f}', \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
